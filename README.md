# Projeto DIO: Organiza√ß√£o e Pesquisa de Documentos com IA - Simula√ß√£o Detalhada

## üìñ Descri√ß√£o do Projeto

Este projeto foi desenvolvido como parte do desafio "Aplicando T√©cnicas de Organiza√ß√£o e Pesquisa de Documentos com Intelig√™ncia Artificial" da [Digital Innovation One (DIO)](https://web.dio.me/). O objetivo principal √© aplicar t√©cnicas de ingest√£o de dados e indexa√ß√£o utilizando ferramentas de intelig√™ncia artificial para organizar e permitir a pesquisa eficiente em um conjunto de documentos. Neste laborat√≥rio simulado, explorei o processo desde a prepara√ß√£o de artigos sobre √âtica em Intelig√™ncia Artificial at√© a extra√ß√£o de conhecimento atrav√©s de um sistema de perguntas e respostas.

## üéØ Objetivos de Aprendizagem

Com a conclus√£o deste desafio (simulado), busquei desenvolver as seguintes habilidades:
* Aplicar conceitos de ingest√£o de dados e indexa√ß√£o com IA em um ambiente pr√°tico.
* Documentar processos t√©cnicos de forma clara, estruturada e detalhada.
* Utilizar o GitHub como ferramenta para versionamento e compartilhamento de documenta√ß√£o t√©cnica e projetos.
* Compreender como ferramentas de IA podem ser utilizadas para minerar e extrair conhecimento de grandes volumes de informa√ß√£o.

## üõ†Ô∏è Ferramentas e Tecnologias Utilizadas

* **Linguagem de Programa√ß√£o:** Python 3.10
* **Ambiente de Desenvolvimento:** Jupyter Notebook (executado via VS Code)
* **Bibliotecas Principais:**
    * **Langchain (v0.1.x):** Para orquestra√ß√£o do fluxo de RAG (Retrieval Augmented Generation).
        * `PyPDFLoader`: Para carregar documentos PDF.
        * `RecursiveCharacterTextSplitter`: Para dividir os textos em chunks menores.
        * `OpenAIEmbeddings`: Para gerar embeddings dos textos.
        * `FAISS`: Para criar e utilizar um √≠ndice vetorial local.
        * `RetrievalQA`: Para construir a cadeia de perguntas e respostas.
        * `OpenAI` (LLM): Para gera√ß√£o das respostas com base nos contextos recuperados.
    * **FAISS-CPU:** Biblioteca para busca de similaridade em vetores, utilizada como vector store local.
    * **OpenAI Python SDK:** Para interagir com os modelos da OpenAI (embeddings e LLM).
    * **Dotenv:** Para gerenciar chaves de API de forma segura.
* **Modelos de Linguagem (LLMs):**
    * `text-embedding-ada-002` (OpenAI): Para gera√ß√£o de embeddings.
    * `gpt-3.5-turbo-instruct` ou `gpt-3.5-turbo` (OpenAI): Para a gera√ß√£o de respostas.
* **Fonte de Dados:** 3 Artigos p√∫blicos em formato PDF sobre √âtica em Intelig√™ncia Artificial.
    * *Exemplo de fontes (hipot√©ticas para esta simula√ß√£o): "The Ethics of Artificial Intelligence" - Nick Bostrom & Eliezer Yudkowsky (Cap√≠tulo de livro), "Asilomar AI Principles" - Future of Life Institute, "Ethical Guidelines for Trustworthy AI" - European Commission.*
* **Versionamento:** Git e GitHub.

*Nota sobre API Keys: Para utilizar os modelos da OpenAI, foi necess√°rio configurar uma API Key. Em um projeto real, √© crucial proteger essa chave, por exemplo, utilizando arquivos `.env` e adicionando `.env` ao `.gitignore`.*

## üöÄ Etapas do Projeto

O projeto foi dividido nas seguintes etapas principais:

### 1. Ingest√£o de Conte√∫do para IA

Nesta etapa, o foco foi preparar e carregar os artigos sobre √©tica em IA para que pudessem ser processados.

* **Descri√ß√£o do Processo:**
    * **Coleta de Dados:** Foram selecionados 3 artigos em formato PDF que abordam diferentes aspectos da √©tica em Intelig√™ncia Artificial. Os arquivos foram salvos em um diret√≥rio local (`./documentos/`).
    * **Carregamento:** Utilizei a classe `PyPDFLoader` da Langchain para carregar o conte√∫do de cada PDF. Cada p√°gina do PDF foi inicialmente carregada como um "Documento" Langchain.
    * **Divis√£o em Chunks (Chunking):** Para otimizar o processo de embedding e a busca de contexto, os textos carregados foram divididos em peda√ßos menores (chunks) utilizando `RecursiveCharacterTextSplitter`. Configurei um `chunk_size` de 1000 caracteres e um `chunk_overlap` de 150 caracteres. O overlap ajuda a manter o contexto entre chunks adjacentes.
* **Ferramentas/Comandos Utilizados (Exemplos de C√≥digo):**
    ```python
    # Exemplo de carregamento de um PDF
    from langchain_community.document_loaders import PyPDFLoader
    loader = PyPDFLoader("./documentos/artigo_etica_ia_01.pdf")
    pages = loader.load()

    # Exemplo de divis√£o em chunks
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    docs = text_splitter.split_documents(pages) # 'pages' seria a jun√ß√£o de todos os documentos carregados
    ```
* **Insights e Desafios:**
    * **Qualidade do PDF:** Percebi que a qualidade do PDF original √© crucial. PDFs que s√£o imagens escaneadas exigiriam OCR (Optical Character Recognition) antes do carregamento, o que n√£o foi o foco aqui, mas √© um ponto de aten√ß√£o.
    * **Ajuste de Chunking:** Experimentar com `chunk_size` e `chunk_overlap` foi importante. Chunks muito pequenos podem perder contexto, enquanto chunks muito grandes podem ser menos eficientes para a busca de similaridade e podem exceder limites de tokens do modelo de embedding ou LLM.
    * **Metadados:** Para esta simula√ß√£o, n√£o adicionei metadados complexos aos chunks, mas percebi que em projetos maiores, adicionar a fonte (nome do arquivo, p√°gina) a cada chunk seria muito √∫til para rastreabilidade.

### 2. Cria√ß√£o de √çndices Inteligentes

Com os dados ingeridos e divididos, o pr√≥ximo passo foi gerar "embeddings" e criar um √≠ndice vetorial para busca sem√¢ntica.

* **Descri√ß√£o do Processo:**
    * **Gera√ß√£o de Embeddings:** Utilizei `OpenAIEmbeddings` (com o modelo `text-embedding-ada-002`) para converter cada chunk de texto em um vetor num√©rico (embedding). Esses vetores capturam o significado sem√¢ntico do texto.
    * **Cria√ß√£o do Vector Store:** Os embeddings gerados, juntamente com os chunks de texto originais, foram armazenados em um √≠ndice vetorial local usando `FAISS`. O FAISS permite buscas r√°pidas por similaridade entre um vetor de consulta e os vetores armazenados.
    * **Persist√™ncia (Opcional, mas recomendado):** Para evitar recriar o √≠ndice a cada execu√ß√£o, o √≠ndice FAISS pode ser salvo localmente (`vectorstore.save_local("faiss_index_etica_ia")`) e carregado posteriormente (`FAISS.load_local(...)`).
* **Ferramentas/Comandos Utilizados (Exemplos de C√≥digo):**
    ```python
    # Configurando API Key (exemplo, melhor usar dotenv)
    # import os
    # os.environ["OPENAI_API_KEY"] = "SUA_CHAVE_API_AQUI"

    from langchain_openai import OpenAIEmbeddings
    from langchain_community.vectorstores import FAISS

    embeddings_model = OpenAIEmbeddings(model="text-embedding-ada-002")

    # Criando o √≠ndice FAISS a partir dos documentos (chunks) e seus embeddings
    # 'docs' s√£o os chunks da etapa anterior
    vectorstore = FAISS.from_documents(docs, embeddings_model)

    # Salvando o √≠ndice (opcional)
    # vectorstore.save_local("faiss_index_etica_ia")
    ```
* **Insights e Desafios:**
    * **Custo de Embeddings:** A gera√ß√£o de embeddings com modelos como o da OpenAI tem um custo associado (embora pequeno para este volume). Em projetos maiores, otimizar o n√∫mero de chunks e escolher modelos de embedding eficientes √© crucial.
    * **Escolha do Vector Store:** FAISS √© excelente para come√ßar e para uso local devido √† sua simplicidade. Para aplica√ß√µes em produ√ß√£o ou com volumes de dados massivos, outras solu√ß√µes como Pinecone, Weaviate ou ChromaDB poderiam ser consideradas, oferecendo mais funcionalidades e escalabilidade.
    * **"Magia" dos Embeddings:** No in√≠cio, o conceito de embeddings pode parecer abstrato, mas ao ver a busca por similaridade funcionando, fica claro o poder de representar texto numericamente de forma sem√¢ntica.

### 3. Explora√ß√£o Pr√°tica dos Dados Organizados

A etapa final consistiu em utilizar o √≠ndice criado para realizar buscas e obter respostas a perguntas sobre os documentos de √©tica em IA.

* **Descri√ß√£o do Processo:**
    * **Configura√ß√£o do Retriever:** O √≠ndice FAISS foi configurado como um `retriever` na Langchain. O retriever √© respons√°vel por buscar os chunks mais relevantes para uma dada consulta. Configurei para retornar os top 3 chunks (`k=3`).
    * **Cadeia de QA (Question Answering):** Utilizei a cadeia `RetrievalQA` da Langchain. Essa cadeia combina o retriever com um modelo de linguagem (LLM, no caso `gpt-3.5-turbo`). O fluxo √©:
        1. A pergunta do usu√°rio √© usada para buscar chunks relevantes no vector store (via retriever).
        2. Os chunks recuperados (contexto) e a pergunta original s√£o passados para o LLM.
        3. O LLM gera uma resposta com base no contexto fornecido.
    * **Realiza√ß√£o de Perguntas:** Formulei perguntas espec√≠ficas sobre o conte√∫do dos artigos de √©tica em IA.
* **Ferramentas/Comandos Utilizados (Exemplos de C√≥digo):**
    ```python
    from langchain_openai import OpenAI
    from langchain.chains import RetrievalQA

    # Carregando o LLM
    llm = OpenAI(temperature=0.2, model_name="gpt-3.5-turbo-instruct") # ou ChatOpenAI para modelos de chat

    # Configurando o retriever a partir do vectorstore
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # Criando a cadeia de QA
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff", # "stuff" simplesmente "enfia" todos os chunks recuperados no prompt
        retriever=retriever,
        return_source_documents=True # Opcional, para ver quais chunks foram usados
    )

    # Exemplo de pergunta
    pergunta = "Quais s√£o os principais princ√≠pios √©ticos para uma IA confi√°vel segundo a Comiss√£o Europeia?"
    resposta = qa_chain.invoke({"query": pergunta})

    print("Pergunta:", pergunta)
    print("Resposta:", resposta["result"])
    # print("Documentos Fonte:", resposta["source_documents"]) # Se return_source_documents=True
    ```
* **Resultados e Insights:**
    * **Qualidade das Respostas:** As respostas geradas pelo LLM foram, em geral, relevantes e baseadas nos trechos recuperados dos PDFs. Por exemplo, ao perguntar sobre "princ√≠pios da IA confi√°vel da Comiss√£o Europeia", o sistema conseguiu extrair e resumir os pontos relevantes do documento correspondente.
    * **Import√¢ncia do Contexto:** A qualidade da resposta depende diretamente da qualidade e relev√¢ncia dos chunks recuperados. Se o retriever n√£o encontrar o contexto correto, o LLM n√£o ter√° base para responder adequadamente ou poder√° "alucinar".
    * **Tipos de Cadeia (`chain_type`):** Experimentei inicialmente com `chain_type="stuff"`, que √© simples mas pode ter limita√ß√µes com contextos muito longos. Para documentos maiores, outros tipos como `map_reduce` ou `refine` seriam mais apropriados.
    * **Engenharia de Prompt (Impl√≠cita):** Embora n√£o tenha customizado extensivamente o prompt da cadeia `RetrievalQA`, percebi que a forma como a pergunta √© feita influencia a busca e a resposta. Perguntas mais espec√≠ficas tendem a gerar resultados melhores.
    * **Limita√ß√µes:** O sistema s√≥ "sabe" o que est√° nos documentos fornecidos. Perguntas fora desse escopo n√£o ser√£o respondidas corretamente.

## üåü Conclus√£o

Este laborat√≥rio simulado foi uma excelente oportunidade para aplicar na pr√°tica os conceitos de ingest√£o, indexa√ß√£o e busca de informa√ß√µes com IA, utilizando a abordagem RAG com Langchain. A capacidade de transformar documentos textuais em uma base de conhecimento pesquis√°vel semanticamente √© poderosa.

Principais aprendizados:
* A import√¢ncia de cada etapa: uma boa ingest√£o e chunking levam a melhores embeddings, que resultam em uma recupera√ß√£o de contexto mais precisa e, consequentemente, respostas mais assertivas do LLM.
* O ecossistema Langchain simplifica enormemente a constru√ß√£o de aplica√ß√µes complexas com LLMs, abstraindo muitas das complexidades.
* Ferramentas como FAISS tornam a cria√ß√£o de √≠ndices vetoriais acess√≠vel mesmo para desenvolvimento local.
* A necessidade de experimenta√ß√£o (tamanho dos chunks, modelos de embedding/LLM, tipos de cadeia) √© fundamental para otimizar o desempenho para um caso de uso espec√≠fico.

Este projeto refor√ßou minha compreens√£o sobre como as ferramentas de IA podem ser utilizadas para minerar e extrair conhecimento valioso de grandes volumes de informa√ß√£o n√£o estruturada.

## ‚ú® Agradecimentos

Agrade√ßo √† DIO pela oportunidade de realizar este desafio e aprender mais sobre o fascinante mundo da Intelig√™ncia Artificial aplicada.
